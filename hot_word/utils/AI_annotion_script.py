import pandas as pd
from tqdm import tqdm
import requests
import os

# TODO: change your dataset path...
CSV_FILE = "../Dataset/chat_data_set/df_0218_all_0205_final.csv"
INDEX_FILE = "../utils/AI_annotation_processed_indices.csv"
OUTPUT_FILE = "../Dataset/AI_annotation_data/ai_0205_final.csv"
BATCH_SIZE = 30000
RESET = False


# åŠ è½½å·²å¤„ç†ç´¢å¼•å‡½æ•°
def load_processed_indices():
    if os.path.exists(INDEX_FILE):
        return pd.read_csv(INDEX_FILE)['index'].tolist()
    return []


# ä¿å­˜å·²å¤„ç†çš„ç´¢å¼•å‡½æ•°
def save_processed_indices(indices):
    pd.DataFrame({'index': indices}).to_csv(INDEX_FILE, index=False)


# å–ä¸‹ä¸€æ‰¹æ ·æœ¬å‡½æ•°
def get_next_batch():
    df = pd.read_csv(CSV_FILE)
    processed = load_processed_indices()

    all_indices = set(df.index.tolist())
    remaining_indices = list(all_indices - set(processed))

    if not remaining_indices:
        print("âœ… æ‰€æœ‰æ ·æœ¬éƒ½å·²å¤„ç†å®Œã€‚")
        return None

    # ä»æœªå¤„ç†çš„ç´¢å¼•ä¸­å–ä¸‹ä¸€æ‰¹
    next_batch_indices = remaining_indices[:BATCH_SIZE]
    batch_df = df.loc[next_batch_indices]

    print(f"ğŸ“¦ æœ¬æ¬¡å–å‡ºæ ·æœ¬æ•°ï¼š{len(batch_df)}")
    return batch_df, processed


def reset_progress():
    """æ¸…é™¤è¿›åº¦æ–‡ä»¶"""
    for file in [INDEX_FILE, OUTPUT_FILE]:
        if os.path.exists(file):
            os.remove(file)
            print(f"ğŸ”„ å·²åˆ é™¤ {file}")
    print("âœ… è¿›åº¦å·²æ¸…é™¤ã€‚")


# è°ƒç”¨AIæ¨¡å‹æ¥å£å‡½æ•°
def request_to_model(query_str, is_classify=False, message=None, temperature=0.5, top_p=0.5):
    """
    è°ƒç”¨glm-4-9b-chatæ¨¡å‹
    :param query_str: è¾“å…¥è¿›å»çš„æ–‡æœ¬å†…å®¹
    :param is_classify:
    :param message:
    :param temperature:
    :param top_p:
    :return: è¿”å›æ¨¡å‹æ ¹æ®æ–‡æœ¬ç»™å‡ºçš„è¾“å‡º
    """
    if message is not None:
        data_config = {
            "model": "glm-4-9b-chat",
            "messages": message,
            "temperature": temperature,
            "top-p": top_p,
            "is_classify": is_classify
        }
    else:
        data_config = {
            "model": "glm-4-9b-chat",
            "messages": query_str,
            "temperature": temperature,
            "top_p": top_p,
            # "max_length" : 4096,
            "is_classify": is_classify
        }

    req = requests.post(
        '',  # TODO: replace your glm key
        json=data_config
    )
    x = req.json()
    return x.get('choices')[0]['message']['content']


# GLM AI æç¤ºè¯ç‰ˆæ ‡æ³¨æ•°æ®
def glm_emotion_annotate(csv_file):
    """
    è‡ªåŠ¨åŒ–æ ‡æ³¨csvé‡Œé¢çš„æ–‡æœ¬æ ‡ç­¾æ•°æ®
    :param csv_file:
    :return: è¿”å›ä¸€ä¸ªæ–°çš„CSVï¼Œå¸¦æœ‰AIæ ‡æ³¨çš„æ ‡ç­¾
    """

    # è¯»å–æ•°æ®
    df_raw, processed = get_next_batch()

    # å°æ‰¹æ¬¡éå†df_raw
    batch_size = 5
    num_batches = (len(df_raw) + batch_size - 1) // batch_size

    # æ˜ å°„è¡¨
    label_map = {'ä¸­æ€§': int(2), 'è´Ÿå‘': int(1), 'æ­£å‘': int(0), 'æœªçŸ¥': None}

    # --ç”Ÿæˆå™¨
    def batch_iter(df, b_s):
        for i in range(0, len(df), b_s):
            yield df.iloc[i:i + b_s]

    # å¯åŠ¨å‰å…ˆåŠ è½½å·²æœ‰æ ‡æ³¨ç»“æœ
    if os.path.exists(OUTPUT_FILE):
        df_ai = pd.read_csv(OUTPUT_FILE)
    else:
        df_ai = pd.DataFrame(columns=['text', 'label'])

    for batch_df in tqdm(batch_iter(df_raw, batch_size), total=num_batches, desc='Processing batches'):
        # å¼€å§‹å¤„ç†æ¯ä¸€ä¸ªæ‰¹æ¬¡çš„dfé€»è¾‘
        pharse = "\n".join([f"{i + 1}. {txt}" for i, txt in enumerate(batch_df['text'])])

        query_str = f"""ä½ æ˜¯ä¸€åèµ„æ·±çš„æ¸¸æˆåˆ¶ä½œä¸»ç®¡,æ·±è°™ä¸­å›½ç©å®¶åœ¨æ¸¸æˆç¤¾åŒº,èŠå¤©ä¸­è¡¨è¾¾æƒ…ç»ªçš„æ–¹å¼,ä¾‹å¦‚ä¸­æ–‡ç©å®¶å¸¸é€šè¿‡å«è“„æˆ–åè®½è¡¨è¾¾æƒ…ç»ªï¼Œä¸è¦åªçœ‹å­—é¢æ„æ€,å¯¹æ··åˆæƒ…ç»ª,ä»¥ä¸»è¦æƒ…æ„Ÿä¸ºå‡†.
        ä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»æˆ‘æä¾›çš„èŠå¤©æ–‡æœ¬,å¹¶æ ¹æ®æ•´ä½“è¯­æ°”åªèƒ½å°†å…¶åˆ’åˆ†ä¸ºä»¥ä¸‹ä¸‰ç±»æƒ…æ„Ÿå…¶ä¸­ä¹‹ä¸€ï¼Œä½ å¿…é¡»åšå‡ºé€‰æ‹©åªèƒ½åˆ’åˆ†ä¸ºå…¶ä¸­ä¸€ä¸ªç±»åˆ«ï¼Œä¸èƒ½å‡ºç°ä¸­æ€§/è´Ÿå‘ç±»ä¼¼è¡¨è¾¾ï¼š
        æ­£å‘ï¼šæ•´ä½“æ˜¯æ»¡æ„,æ„‰å¿«,èµæ‰¬,ç§¯æçš„
        ä¸­æ€§ï¼šæ²¡æœ‰æ˜æ˜¾æƒ…ç»ª,æˆ–è€…åªæ˜¯æè¿°äº‹å®
        è´Ÿå‘ï¼šæ•´ä½“æ˜¯ä¸æ»¡ï¼Œå¤±æœ›ï¼Œæ„¤æ€’ï¼Œè®½åˆºï¼ŒæŠ±æ€¨
        è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡º:
1.åˆ†ç±»: æ­£å‘ | ç†ç”±: ...
2.åˆ†ç±»: è´Ÿå‘ | ç†ç”±: ...
3.åˆ†ç±»: ä¸­æ€§ | ç†ç”±: ...
4.åˆ†ç±»: ä¸­æ€§ | ç†ç”±: ...
5.åˆ†ç±»: è´Ÿå‘ | ç†ç”±: ...

        æ–‡æœ¬å¦‚ä¸‹ï¼š
{pharse}
        """
        response = request_to_model(query_str, is_classify=False)

        # è§£ææ¨¡å‹è¿”å›ç»“æœ
        lines = response.strip().split("\n")
        batch_results = []

        # æ¨¡å‹è¾“å‡ºå¯èƒ½ä¼šè¾“å‡ºç©ºè¡Œ,å¤šè¡Œï¼ˆä¾‹å¦‚ç†ç”±è¡Œä¹Ÿä½œä¸ºä¸€è¡Œ)è¦è¿›è¡Œæ¸…æ´—é€»è¾‘
        valid_lines = [line for line in lines if "åˆ†ç±»" in line]
        valid_lines = valid_lines[-5:]

        for i, line in enumerate(valid_lines):
            try:
                label_part = line.split("åˆ†ç±»:")[1].split("|")[0].strip()
            except IndexError:
                label_part = "æœªçŸ¥"

            batch_results.append({
                'text': batch_df.iloc[i]['text'],
                'label': label_map[label_part]
            })
        print("##########")
        print(response)
        # print(batch_df)
        # ç›´æ¥åˆå¹¶å½“å‰æ‰¹æ¬¡
        df_ai = pd.concat([df_ai, pd.DataFrame(batch_results)], ignore_index=True)

        # å»é‡é¿å…é‡å¤ä¿å­˜
        df_ai = df_ai.drop_duplicates(subset=['text'], keep='last')

        # æ›´æ–°è¿›åº¦(éœ€è¦æ—¶åˆ»æ›´æ–°å·²ç»å¤„ç†å®Œçš„ç´¢å¼•,è¿˜éœ€è¦æ­£ç¡®æ›´æ–°processed)
        processed = processed + batch_df.index.tolist()
        save_processed_indices(processed)
        print("è¿›åº¦å·²æ›´æ–°")

        # ä¿å­˜è¿›åº¦
        df_ai.to_csv(OUTPUT_FILE, index=False)
        print(f"ğŸ’¾ å·²ç´¯è®¡ä¿å­˜ {len(df_ai)} æ¡æ ‡æ³¨æ•°æ®åˆ° {OUTPUT_FILE}")
        # print(df_annotated)
        # break


# è±†åŒ…API
def request_doubao(self, content):
    from openai import OpenAI

    # openai_api_key = "d1b18464-b9cf-4b7e-9a19-6a17b37ee74c"
    openai_api_key = ""  # TODO: replace your api key
    client = OpenAI(
        api_key=openai_api_key,
        base_url="",  # TODO: replace your api base_url
    )

    # Non-streaming:
    messages = [{"role": "user", "content": "{}".format(content)}]
    completion = client.chat.completions.create(
        # model="ep-20250212103518-lhkk7",  # your model endpoint ID
        # model="ep-20250402144717-grkgt",  # your model endpoint ID
        model="ep-20250402113148-cdzlc",  # your model endpoint ID
        messages=messages,
        stream=False,  # å…³é”®å‚æ•°å¯ç”¨æµå¼
    )
    res_content = completion.choices[0].message.content

    # print("content:", res_content)
    return res_content


if __name__ == "__main__":
    if RESET:
        reset_progress()
    else:
        glm_emotion_annotate(CSV_FILE)

    print("end")